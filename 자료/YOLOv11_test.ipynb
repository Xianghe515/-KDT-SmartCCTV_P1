{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\tkdgu\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.35M/5.35M [00:00<00:00, 15.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.89  Python-3.12.8 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i7-1360P)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.6.0+cpu...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success  1.6s, saved as 'yolo11n.torchscript' (10.5 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['ncnn'] not found, attempting AutoUpdate...\n",
      "Collecting ncnn\n",
      "  Downloading ncnn-1.0.20241226-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: numpy in d:\\kim\\yolo11\\venv\\lib\\site-packages (from ncnn) (2.1.1)\n",
      "Requirement already satisfied: tqdm in d:\\kim\\yolo11\\venv\\lib\\site-packages (from ncnn) (4.67.1)\n",
      "Requirement already satisfied: requests in d:\\kim\\yolo11\\venv\\lib\\site-packages (from ncnn) (2.32.3)\n",
      "Collecting portalocker (from ncnn)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: opencv-python in d:\\kim\\yolo11\\venv\\lib\\site-packages (from ncnn) (4.11.0.86)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\kim\\yolo11\\venv\\lib\\site-packages (from portalocker->ncnn) (309)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\kim\\yolo11\\venv\\lib\\site-packages (from requests->ncnn) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\kim\\yolo11\\venv\\lib\\site-packages (from requests->ncnn) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\kim\\yolo11\\venv\\lib\\site-packages (from requests->ncnn) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\kim\\yolo11\\venv\\lib\\site-packages (from requests->ncnn) (2025.1.31)\n",
      "Requirement already satisfied: colorama in d:\\kim\\yolo11\\venv\\lib\\site-packages (from tqdm->ncnn) (0.4.6)\n",
      "Downloading ncnn-1.0.20241226-cp312-cp312-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 4.0/4.0 MB 14.9 MB/s eta 0:00:00\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: portalocker, ncnn\n",
      "Successfully installed ncnn-1.0.20241226 portalocker-3.1.1\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success  2.2s, installed 1 package: ['ncnn']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mNCNN:\u001b[0m starting export with NCNN 1.0.20241226...\n",
      "\u001b[34m\u001b[1mNCNN:\u001b[0m WARNING  PNNX not found. Attempting to download binary file from https://github.com/pnnx/pnnx/.\n",
      "Note PNNX Binary file must be placed in current working directory or in D:\\kim\\Yolo11\\venv\\Lib\\site-packages\\ultralytics. See PNNX repo for full installation instructions.\n",
      "\u001b[34m\u001b[1mNCNN:\u001b[0m successfully found latest PNNX asset file pnnx-20241223-windows.zip\n",
      "Downloading https://github.com/pnnx/pnnx/releases/download/20241223/pnnx-20241223-windows.zip to 'pnnx-20241223-windows.zip'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16.0M/16.0M [00:00<00:00, 28.9MB/s]\n",
      "Unzipping pnnx-20241223-windows.zip to D:\\kim\\Yolo11\\pnnx-20241223-windows...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 13.51file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mNCNN:\u001b[0m running 'D:\\kim\\Yolo11\\venv\\Lib\\site-packages\\ultralytics\\pnnx.exe yolo11n.torchscript ncnnparam=yolo11n_ncnn_model\\model.ncnn.param ncnnbin=yolo11n_ncnn_model\\model.ncnn.bin ncnnpy=yolo11n_ncnn_model\\model_ncnn.py pnnxparam=yolo11n_ncnn_model\\model.pnnx.param pnnxbin=yolo11n_ncnn_model\\model.pnnx.bin pnnxpy=yolo11n_ncnn_model\\model_pnnx.py pnnxonnx=yolo11n_ncnn_model\\model.pnnx.onnx fp16=0 device=cpu inputshape=\"[1, 3, 640, 640]\"'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mNCNN:\u001b[0m export success  6.2s, saved as 'yolo11n_ncnn_model' (10.2 MB)\n",
      "\n",
      "Export complete (10.0s)\n",
      "Results saved to \u001b[1mD:\\kim\\Yolo11\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n_ncnn_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n_ncnn_model imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "WARNING  Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Export the model to NCNN format\n",
    "model.export(format=\"ncnn\")  # creates '/yolo11n_ncnn_model'\n",
    "\n",
    "# Load the exported NCNN model\n",
    "ncnn_model = YOLO(\"./yolo11n_ncnn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.89 üöÄ Python-3.12.8 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i7-1360P)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "Downloading https://ultralytics.com/images/zidane.jpg to 'zidane.jpg'...\n",
      "image 1/1 d:\\kim\\Yolo11\\zidane.jpg: 384x640 2 persons, 1 tie, 74.8ms\n",
      "Speed: 3.6ms preprocess, 74.8ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/49.2k [00:00<?, ?B/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49.2k/49.2k [00:00<00:00, 1.51MB/s]\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model=yolo11n.pt source='https://ultralytics.com/images/zidane.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.89 üöÄ Python-3.12.8 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i7-1360P)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "image 1/1 D:\\kim\\Yolo11\\sample\\Ï¶ùÎ™ÖÏÇ¨ÏßÑ.jpg: 640x512 1 person, 115.2ms\n",
      "Speed: 3.2ms preprocess, 115.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Results saved to \u001b[1mruns\\detect\\predict2\u001b[0m\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: SyntaxWarning: invalid escape sequence '\\k'\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model=yolo11n.pt source='D:\\kim\\Yolo11\\sample\\Ï¶ùÎ™ÖÏÇ¨ÏßÑ.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 876.2ms\n",
      "Speed: 2.4ms preprocess, 876.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1174.7ms\n",
      "Speed: 1.4ms preprocess, 1174.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 728.8ms\n",
      "Speed: 1.0ms preprocess, 728.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 727.5ms\n",
      "Speed: 1.0ms preprocess, 727.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 522.9ms\n",
      "Speed: 1.0ms preprocess, 522.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 322.7ms\n",
      "Speed: 1.4ms preprocess, 322.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 242.4ms\n",
      "Speed: 1.0ms preprocess, 242.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 315.6ms\n",
      "Speed: 1.0ms preprocess, 315.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 403.6ms\n",
      "Speed: 1.0ms preprocess, 403.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 261.5ms\n",
      "Speed: 1.4ms preprocess, 261.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 307.8ms\n",
      "Speed: 1.1ms preprocess, 307.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 236.1ms\n",
      "Speed: 1.0ms preprocess, 236.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 282.3ms\n",
      "Speed: 0.9ms preprocess, 282.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 331.7ms\n",
      "Speed: 1.2ms preprocess, 331.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 298.0ms\n",
      "Speed: 1.3ms preprocess, 298.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 272.6ms\n",
      "Speed: 1.1ms preprocess, 272.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 252.6ms\n",
      "Speed: 1.2ms preprocess, 252.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 279.8ms\n",
      "Speed: 1.0ms preprocess, 279.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 296.9ms\n",
      "Speed: 1.1ms preprocess, 296.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 368.6ms\n",
      "Speed: 1.1ms preprocess, 368.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 288.4ms\n",
      "Speed: 1.1ms preprocess, 288.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 230.6ms\n",
      "Speed: 1.0ms preprocess, 230.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 278.0ms\n",
      "Speed: 1.0ms preprocess, 278.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 278.6ms\n",
      "Speed: 1.9ms preprocess, 278.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 240.5ms\n",
      "Speed: 1.2ms preprocess, 240.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 329.1ms\n",
      "Speed: 1.1ms preprocess, 329.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 252.4ms\n",
      "Speed: 1.2ms preprocess, 252.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 261.2ms\n",
      "Speed: 1.1ms preprocess, 261.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 311.8ms\n",
      "Speed: 1.9ms preprocess, 311.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 224.0ms\n",
      "Speed: 1.4ms preprocess, 224.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 332.2ms\n",
      "Speed: 1.6ms preprocess, 332.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 270.6ms\n",
      "Speed: 1.0ms preprocess, 270.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 269.2ms\n",
      "Speed: 1.0ms preprocess, 269.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 278.4ms\n",
      "Speed: 0.9ms preprocess, 278.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 299.8ms\n",
      "Speed: 1.0ms preprocess, 299.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 201.0ms\n",
      "Speed: 0.9ms preprocess, 201.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 304.0ms\n",
      "Speed: 0.9ms preprocess, 304.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 245.1ms\n",
      "Speed: 1.3ms preprocess, 245.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 298.0ms\n",
      "Speed: 0.9ms preprocess, 298.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 307.6ms\n",
      "Speed: 1.2ms preprocess, 307.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 304.1ms\n",
      "Speed: 1.0ms preprocess, 304.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 328.0ms\n",
      "Speed: 1.3ms preprocess, 328.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 425.6ms\n",
      "Speed: 1.3ms preprocess, 425.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 431.3ms\n",
      "Speed: 1.3ms preprocess, 431.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 471.7ms\n",
      "Speed: 1.2ms preprocess, 471.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 493.8ms\n",
      "Speed: 2.1ms preprocess, 493.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 402.1ms\n",
      "Speed: 1.8ms preprocess, 402.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 449.0ms\n",
      "Speed: 1.6ms preprocess, 449.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 357.4ms\n",
      "Speed: 1.4ms preprocess, 357.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 376.3ms\n",
      "Speed: 2.0ms preprocess, 376.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 512.2ms\n",
      "Speed: 1.5ms preprocess, 512.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 418.1ms\n",
      "Speed: 2.1ms preprocess, 418.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 345.4ms\n",
      "Speed: 2.2ms preprocess, 345.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 379.7ms\n",
      "Speed: 2.0ms preprocess, 379.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 434.3ms\n",
      "Speed: 1.4ms preprocess, 434.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 443.3ms\n",
      "Speed: 2.0ms preprocess, 443.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 489.2ms\n",
      "Speed: 2.8ms preprocess, 489.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 353.4ms\n",
      "Speed: 1.4ms preprocess, 353.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 255.8ms\n",
      "Speed: 1.1ms preprocess, 255.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 312.3ms\n",
      "Speed: 1.3ms preprocess, 312.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 299.2ms\n",
      "Speed: 1.1ms preprocess, 299.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 418.7ms\n",
      "Speed: 1.5ms preprocess, 418.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 333.4ms\n",
      "Speed: 2.3ms preprocess, 333.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 237.1ms\n",
      "Speed: 1.0ms preprocess, 237.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 271.8ms\n",
      "Speed: 1.0ms preprocess, 271.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 222.6ms\n",
      "Speed: 1.3ms preprocess, 222.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 311.3ms\n",
      "Speed: 1.2ms preprocess, 311.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 laptop, 301.7ms\n",
      "Speed: 1.1ms preprocess, 301.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 laptop, 290.5ms\n",
      "Speed: 1.4ms preprocess, 290.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 247.6ms\n",
      "Speed: 1.6ms preprocess, 247.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 327.1ms\n",
      "Speed: 1.4ms preprocess, 327.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 322.1ms\n",
      "Speed: 1.4ms preprocess, 322.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 274.7ms\n",
      "Speed: 1.6ms preprocess, 274.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 371.5ms\n",
      "Speed: 1.3ms preprocess, 371.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 265.2ms\n",
      "Speed: 1.1ms preprocess, 265.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 laptop, 222.7ms\n",
      "Speed: 1.2ms preprocess, 222.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 laptop, 284.5ms\n",
      "Speed: 1.3ms preprocess, 284.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 laptop, 265.2ms\n",
      "Speed: 1.1ms preprocess, 265.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "class YOLODetector:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        YOLODetector Ï¥àÍ∏∞Ìôî\n",
    "        :model_path: YOLO Î™®Îç∏ Í≤ΩÎ°ú\n",
    "        :webcam_index: ÏõπÏ∫† Ïù∏Îç±Ïä§\n",
    "        \"\"\"\n",
    "        self.model_path = \"yolo11m.pt\"\n",
    "        self.webcam_idx = 0\n",
    "        \n",
    "        self.model = YOLO(self.model_path)\n",
    "        self.cap = cv2.VideoCapture(self.webcam_idx)\n",
    "\n",
    "        if not self.cap.isOpened():\n",
    "            raise ValueError(\"ÏõπÏ∫†Ïù¥ Ïó∞Í≤∞ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.\")\n",
    "        \n",
    "        # Í∞Å ÌÅ¥ÎûòÏä§Ïóê Í≥†Ïú†Ìïú ÏÉâÏÉÅÏúºÎ°ú Í≤ΩÍ≥Ñ ÏÉÅÏûêÎ•º Í∑∏Î¶º\n",
    "        self.t_prev = 0\n",
    "        self.colors = np.random.uniform(0, 255, size=(len(self.model.names), 3))\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            ret, frame = self.cap.read()\n",
    "            if not ret:\n",
    "                print(\"FrameÏùÑ ÏùΩÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "                break\n",
    "\n",
    "            img = frame.copy()\n",
    "\n",
    "            results = self.model(img)\n",
    "\n",
    "            for result in results:\n",
    "                for box in result.boxes:\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()  # BBox ÏΩîÎîîÎÑ§Ïù¥Ìä∏Ìä∏\n",
    "                    \n",
    "                    conf = box.conf[0].item()  # Ïª®ÌîºÎçòÏä§Ïä§\n",
    "                    cls = box.cls[0].item()  # ÌÅ¥ÎûòÏä§ ID\n",
    "                    class_name = self.model.names[int(cls)]  # ÌÅ¥ÎûòÏä§ Î™Ö\n",
    "\n",
    "                    # 0.65% Ïù¥ÏÉÅÏùº Îïå ÌëúÏãú\n",
    "                    if conf >= 0.65:\n",
    "                        color = self.colors[int(cls)]\n",
    "                        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)\n",
    "                        cv2.putText(\n",
    "                            img,\n",
    "                            f\"{class_name} {conf:.2f}\",\n",
    "                            (int(x1), int(y1) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.8,\n",
    "                            color,\n",
    "                            3,\n",
    "                        )\n",
    "\n",
    "            # ÏãúÍ∞ÅÌôî\n",
    "            cv2.imshow(\"YOLO Detection\", img)\n",
    "            cv2.imshow(\"Raw Image\", frame)\n",
    "\n",
    "            # 'q' ÌÇ§Î•º ÎàÑÎ•¥Î©¥ Ï¢ÖÎ£å\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = YOLODetector()\n",
    "    detector.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ncnn Ï†ÅÏö© -> Ï∂úÎ†• ÏÜçÎèÑ Ìñ•ÏÉÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 chair, 142.0ms\n",
      "Speed: 2.4ms preprocess, 142.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 228.8ms\n",
      "Speed: 2.0ms preprocess, 228.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 114.2ms\n",
      "Speed: 1.2ms preprocess, 114.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 115.4ms\n",
      "Speed: 0.9ms preprocess, 115.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 124.2ms\n",
      "Speed: 0.8ms preprocess, 124.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 203.3ms\n",
      "Speed: 1.0ms preprocess, 203.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 133.8ms\n",
      "Speed: 1.8ms preprocess, 133.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 212.5ms\n",
      "Speed: 1.0ms preprocess, 212.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 207.7ms\n",
      "Speed: 1.0ms preprocess, 207.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 193.5ms\n",
      "Speed: 0.8ms preprocess, 193.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 155.9ms\n",
      "Speed: 1.1ms preprocess, 155.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 77.0ms\n",
      "Speed: 1.0ms preprocess, 77.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 125.5ms\n",
      "Speed: 1.0ms preprocess, 125.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 118.6ms\n",
      "Speed: 1.2ms preprocess, 118.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 94.9ms\n",
      "Speed: 1.1ms preprocess, 94.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 56.2ms\n",
      "Speed: 1.4ms preprocess, 56.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 52.3ms\n",
      "Speed: 1.4ms preprocess, 52.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 78.0ms\n",
      "Speed: 1.3ms preprocess, 78.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 101.0ms\n",
      "Speed: 1.0ms preprocess, 101.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 chair, 57.9ms\n",
      "Speed: 1.3ms preprocess, 57.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 46.2ms\n",
      "Speed: 0.9ms preprocess, 46.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 92.6ms\n",
      "Speed: 0.9ms preprocess, 92.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 117.9ms\n",
      "Speed: 1.1ms preprocess, 117.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 136.9ms\n",
      "Speed: 0.9ms preprocess, 136.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 82.8ms\n",
      "Speed: 1.1ms preprocess, 82.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 102.0ms\n",
      "Speed: 1.7ms preprocess, 102.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 66.6ms\n",
      "Speed: 1.8ms preprocess, 66.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 48.5ms\n",
      "Speed: 0.9ms preprocess, 48.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 47.3ms\n",
      "Speed: 1.3ms preprocess, 47.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 106.9ms\n",
      "Speed: 1.1ms preprocess, 106.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 56.3ms\n",
      "Speed: 1.1ms preprocess, 56.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 49.7ms\n",
      "Speed: 1.2ms preprocess, 49.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 77.5ms\n",
      "Speed: 1.0ms preprocess, 77.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 167.6ms\n",
      "Speed: 1.1ms preprocess, 167.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 57.8ms\n",
      "Speed: 1.1ms preprocess, 57.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 52.1ms\n",
      "Speed: 1.3ms preprocess, 52.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 78.1ms\n",
      "Speed: 1.0ms preprocess, 78.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 89.3ms\n",
      "Speed: 1.1ms preprocess, 89.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 84.1ms\n",
      "Speed: 1.4ms preprocess, 84.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 58.5ms\n",
      "Speed: 1.2ms preprocess, 58.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 100.9ms\n",
      "Speed: 1.0ms preprocess, 100.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 102.7ms\n",
      "Speed: 0.9ms preprocess, 102.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 49.3ms\n",
      "Speed: 1.2ms preprocess, 49.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 139.8ms\n",
      "Speed: 1.0ms preprocess, 139.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 51.7ms\n",
      "Speed: 1.3ms preprocess, 51.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 47.0ms\n",
      "Speed: 1.1ms preprocess, 47.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 46.4ms\n",
      "Speed: 0.9ms preprocess, 46.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 48.0ms\n",
      "Speed: 1.0ms preprocess, 48.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 113.6ms\n",
      "Speed: 1.0ms preprocess, 113.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 76.1ms\n",
      "Speed: 1.1ms preprocess, 76.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 52.3ms\n",
      "Speed: 1.1ms preprocess, 52.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 74.8ms\n",
      "Speed: 1.1ms preprocess, 74.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 99.1ms\n",
      "Speed: 1.1ms preprocess, 99.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 90.7ms\n",
      "Speed: 1.0ms preprocess, 90.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 54.4ms\n",
      "Speed: 1.6ms preprocess, 54.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 55.7ms\n",
      "Speed: 1.4ms preprocess, 55.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 50.2ms\n",
      "Speed: 1.2ms preprocess, 50.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 99.4ms\n",
      "Speed: 0.9ms preprocess, 99.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 94.3ms\n",
      "Speed: 1.1ms preprocess, 94.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 76.5ms\n",
      "Speed: 1.0ms preprocess, 76.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "class YOLODetector:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        YOLODetector Ï¥àÍ∏∞Ìôî\n",
    "        :model_path: YOLO Î™®Îç∏ Í≤ΩÎ°ú\n",
    "        :webcam_index: ÏõπÏ∫† Ïù∏Îç±Ïä§\n",
    "        \"\"\"\n",
    "        self.model_path = \"yolo11n.pt\"\n",
    "        self.webcam_idx = 0\n",
    "        \n",
    "        self.ncnn_model = YOLO(self.model_path)\n",
    "        self.cap = cv2.VideoCapture(self.webcam_idx)\n",
    "\n",
    "        if not self.cap.isOpened():\n",
    "            raise ValueError(\"ÏõπÏ∫†Ïù¥ Ïó∞Í≤∞ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.\")\n",
    "        \n",
    "        self.t_prev = 0\n",
    "        self.colors = np.random.uniform(0, 255, size=(len(self.ncnn_model.names), 3))\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            ret, frame = self.cap.read()\n",
    "            if not ret:\n",
    "                print(\"FrameÏùÑ ÏùΩÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "                break\n",
    "\n",
    "            img = frame.copy()\n",
    "\n",
    "            results = self.ncnn_model(img)\n",
    "\n",
    "            for result in results:\n",
    "                for box in result.boxes:\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                    \n",
    "                    conf = box.conf[0].item()\n",
    "                    cls = box.cls[0].item()\n",
    "                    class_name = self.ncnn_model.names[int(cls)]\n",
    "\n",
    "                    if conf >= 0.65:\n",
    "                        color = self.colors[int(cls)]\n",
    "                        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)\n",
    "                        cv2.putText(\n",
    "                            img,\n",
    "                            f\"{class_name} {conf:.2f}\",\n",
    "                            (int(x1), int(y1) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.8,\n",
    "                            color,\n",
    "                            3,\n",
    "                        )\n",
    "\n",
    "            # ÏãúÍ∞ÅÌôî\n",
    "            cv2.imshow(\"YOLO Detection\", img)\n",
    "            cv2.imshow(\"Raw Image\", frame)\n",
    "\n",
    "            # 'q' ÌÇ§Î•º ÎàÑÎ•¥Î©¥ Ï¢ÖÎ£å\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = YOLODetector()\n",
    "    detector.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
